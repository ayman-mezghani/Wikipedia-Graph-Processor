{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('JAVA_HOME', '/usr/lib/jvm/java-1.8.0-openjdk-amd64')\n",
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import pickle\n",
    "import pyspark.ml\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from neo4j import GraphDatabase, Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_graph(path):\n",
    "    print('importing graph from :', path)\n",
    "    g = nx.read_gexf(path, node_type=None, relabel=True).to_undirected()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_graph(g):\n",
    "    print('number of nodes : ' + str(len(g)))\n",
    "    fig, ax = plt.subplots(figsize=(100, 70)) # set size\n",
    "    nx.draw(g, with_labels=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_graph(graph):\n",
    "    print('cleaning graph')\n",
    "    #Degree computation\n",
    "    nodes_with_degrees = graph.degree\n",
    "    #mean\n",
    "    mean_deg = statistics.mean(l[1] for l in nodes_with_degrees)\n",
    "    #std\n",
    "    std_deg = statistics.stdev(l[1] for l in nodes_with_degrees)\n",
    "    #threshold\n",
    "    threshold = mean_deg + std_deg*std_deg/2 \n",
    "\n",
    "    #Filtering extremely highly connected nodes\n",
    "    nodes_to_remove = list(filter(lambda d : d[1] > threshold, nodes_with_degrees))\n",
    "    n,d = zip(*nodes_to_remove)\n",
    "    graph.remove_nodes_from(n)\n",
    "    \n",
    "    #Filtering isolated nodes\n",
    "    nodes_with_degrees = graph.degree\n",
    "    nodes_to_remove = list(filter(lambda d : d[1] == 0, nodes_with_degrees))\n",
    "    n,d = zip(*nodes_to_remove)\n",
    "    graph.remove_nodes_from(n)\n",
    "\n",
    "    print('remaining nodes : ' + str(len(graph)))\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only largest connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_connected_component(graph):\n",
    "    print('largest connected component')\n",
    "    gcc = max(nx.connected_component_subgraphs(graph), key=len)\n",
    "    print('remaining nodes : ' + str(len(gcc)))\n",
    "    return gcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning using Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communities_louvain(graph):\n",
    "    louvain_communities = community.best_partition(graph, resolution=1)\n",
    "    louvain_communities_dict = {}\n",
    "    for key, value in sorted(louvain_communities.items()):\n",
    "        louvain_communities_dict.setdefault(value, []).append(key)\n",
    "\n",
    "    print('detcted',len(louvain_communities_dict),'communities')\n",
    "    \n",
    "    return louvain_communities_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning using Leiden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/leidenalg/\n",
    "https://www.nature.com/articles/s41598-019-41695-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categoriy of each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=('',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of categories of a page\n",
    "def get_categories(page_name):\n",
    "    c = list()\n",
    "    with driver.session() as session:\n",
    "        with session.begin_transaction() as tx:\n",
    "            for record in tx.run(\"MATCH (p:Page)-[:BELONGS_TO]->(c:Category) \"\n",
    "                                 \"WHERE p.title = {page_name} \"\n",
    "                                 \"AND NOT exists((c)-[:BELONGS_TO]->(:Category {title: \\'Hidden_categories\\'})) \"\n",
    "                                 \"RETURN c.title\", \n",
    "                                 page_name = page_name ):\n",
    "                #print(record[\"c.title\"])\n",
    "                c.append(record[\"c.title\"])\n",
    "    return c\n",
    "\n",
    "#map each element to frequency in a list    \n",
    "def count_frequency(my_list): \n",
    "      \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for items in my_list: \n",
    "        freq[items] = my_list.count(items)\n",
    "    return freq\n",
    "\n",
    "#iterate over pages dict partition\n",
    "def part_category_fetch(key, dic):\n",
    "    cat = []\n",
    "    for title in dic[key]:\n",
    "        cat += get_categories(title)\n",
    "    print('done fetching')\n",
    "    return cat\n",
    "\n",
    "def fetcher(bpd):\n",
    "    part_cat = {}\n",
    "    \n",
    "    for part in sorted(bpd):\n",
    "        print(part)\n",
    "        cat = part_category_fetch(part, bpd)\n",
    "        #print(cat)\n",
    "        part_cat.setdefault(part, cat)\n",
    "    \n",
    "    return part_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch categories for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_categories(bpd):\n",
    "    part_cat_dict = fetcher(bpd)\n",
    "    \n",
    "    return part_cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_frequencies(d):\n",
    "    part_cat_dict_freq = {}\n",
    "    for e in d:\n",
    "        cat_map_freq = count_frequency(part_cat_dict[e])\n",
    "        part_cat_dict_freq.setdefault(e, cat_map_freq)\n",
    "    return part_cat_dict_freq\n",
    " \n",
    "def find_max_freq(p):\n",
    "    max_part_cat = {}\n",
    "    for e in p:\n",
    "        ls = list(p[e].keys())\n",
    "        cat = ls[0]\n",
    "        for x in ls:\n",
    "            if p[e][cat] < p[e][x]:\n",
    "                cat = x\n",
    "        max_part_cat.setdefault(e, cat)\n",
    "    return max_part_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = import_graph(\"graphs/peaks_graph_20190901_20190915.gexf\")\n",
    "verify_graph(g)\n",
    "gg = largest_connected_component(clean_graph(g))\n",
    "verify_graph(gg)\n",
    "communities = communities_louvain(gg)\n",
    "part_cat_dict = fetch_categories(communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(part_cat_dict[len(part_cat_dict)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute frequency of category in each part and find max freq cat for each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxx = find_max_freq(count_all_frequencies(part_cat_dict))\n",
    "for e in sorted(maxx):\n",
    "    print(e, maxx[e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('LDA').config(\"spark.master\", \"local[*]\").config(\"spark.sql.warehouse.dir\", \"/home/ayman/warehouse\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(part_cat_dict[len(part_cat_dict) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = []\n",
    "for p in part_cat_dict:\n",
    "    df_.append((p, ' '.join(part_cat_dict[p]).replace('_', ' ').replace(',','').replace('\\\\\\'', ' ').lower()))\n",
    "        \n",
    "for e in df_:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionsData = spark.createDataFrame(df_, ['cluster', 'categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(partitionsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"categories\", outputCol=\"raw\")\n",
    "wordsDirtyData = tokenizer.transform(partitionsData)\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"wordsU\")\n",
    "wordsDataUnstemmed = remover.transform(wordsDirtyData)\n",
    "print(wordsDataUnstemmed)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_udf = udf(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens], ArrayType(StringType()))\n",
    "wordsData = wordsDataUnstemmed.withColumn(\"words\", lemmatizer_udf(\"wordsU\")).select('cluster', 'categories', 'raw', 'words')\n",
    "print(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordsData.select('raw').first())\n",
    "print(wordsData.select('words').first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "#featurizedData = hashingTF.transform(wordsData)\n",
    "#print(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "cvmodel = countVectorizer.fit(wordsData)\n",
    "featurizedData = cvmodel.transform(wordsData)\n",
    "print(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "print(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = rescaledData.select('cluster', 'features')\n",
    "print(dataset)\n",
    "dataset.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trains a LDA model.\n",
    "lda = LDA(k=len(part_cat_dict), maxIter=100)\n",
    "ldaModel = lda.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ll = ldaModel.logLikelihood(dataset)\n",
    "lp = ldaModel.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = ldaModel.describeTopics(5)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show()\n",
    "\n",
    "# Shows the result\n",
    "transformed = ldaModel.transform(dataset)\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification for one cluster:\n",
    "getting the full row, getting the topic index and the corresponding topic informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = transformed.select('topicDistribution').first()[0]\n",
    "print(transformed.first())\n",
    "m = list(l).index(max(l))\n",
    "print('\\ntopic index is :',m)\n",
    "print(topics.take(m+1)[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the 5 most relevant words and their weights for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)\n",
    "vocabList = cvmodel.vocabulary\n",
    "tops = []\n",
    "for i,t,w in topicIndices.collect():\n",
    "    print('Topic %d:' % i)\n",
    "    entry = []\n",
    "    for j in range(len(t)):\n",
    "        entry.append(vocabList[t[j]])\n",
    "        #print('\\t', vocabList[t[j]], w[j])\n",
    "    print(entry)\n",
    "    tops.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/44233862/visualizing-topics-with-spark-lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing array of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign topic words to each cluster using LDA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_topicDist = sorted(transformed.select('cluster', 'topicDistribution').collect())\n",
    "cluster_topicTerms = []\n",
    "for e in cluster_topicDist:\n",
    "    m = list(e[1]).index(max(e[1]))\n",
    "    print(m)\n",
    "    cluster_topicTerms.append(tops[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DF with cluster number, its pages, the LDAtopics and the MAXcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_ = []\n",
    "for k in sorted(communities.keys()):\n",
    "    df2_.append((k, communities[k], cluster_topicTerms[k], maxx[k]))\n",
    "    \n",
    "partitionsData2 = spark.createDataFrame(df2_, ['cluster', 'page names', 'LDAtopics', 'MAXcategory'])\n",
    "partitionsData2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distance between words using how many letters to flip<br>\n",
    "checking the connected clusters (are they similar ?)<br>\n",
    "stemming<br>\n",
    "display the title of the most viewed page (in each cluster)<br>\n",
    "find central node in each cluster (node_centrality)<br>\n",
    "visualization<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
