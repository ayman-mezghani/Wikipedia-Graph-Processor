{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('JAVA_HOME', '/usr/lib/jvm/java-1.8.0-openjdk-amd64')\n",
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import wikipediaapi\n",
    "import pickle\n",
    "import pyspark.ml\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from neo4j import GraphDatabase, Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"peaks_graph_20190901_20190915.gexf\"\n",
    "g = nx.read_gexf(path, node_type=None, relabel=True).to_undirected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of nodes : ' + str(len(g)))\n",
    "fig, ax = plt.subplots(figsize=(30, 20)) # set size\n",
    "nx.draw(g, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree computation + mean + std + threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_degrees = g.degree\n",
    "\n",
    "mean_deg = statistics.mean(l[1] for l in nodes_with_degrees)\n",
    "\n",
    "std_deg = statistics.stdev(l[1] for l in nodes_with_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering extremely highly connected nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = mean_deg + std_deg*std_deg/2 \n",
    "\n",
    "nodes_to_remove = list(filter(lambda d : d[1] > threshold, nodes_with_degrees))\n",
    "\n",
    "n,d = zip(*nodes_to_remove)\n",
    "\n",
    "g.remove_nodes_from(n)\n",
    "print('remaining nodes : ' + str(len(g)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degree computation + filtering isolated nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_degrees = g.degree\n",
    "\n",
    "nodes_to_remove = list(filter(lambda d : d[1] == 0, nodes_with_degrees))\n",
    "\n",
    "n,d = zip(*nodes_to_remove)\n",
    "\n",
    "g.remove_nodes_from(n)\n",
    "\n",
    "print('remaining nodes : ' + str(len(g)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only largest connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc = max(nx.connected_component_subgraphs(g), key=len)\n",
    "\n",
    "print('remaining nodes : ' + str(len(gcc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(100, 70)) # set size\n",
    "nx.draw(gcc, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning using Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_communities = community.best_partition(gcc, resolution=1)\n",
    "\n",
    "\n",
    "louvain_communities_dict = {}\n",
    "\n",
    "for key, value in sorted(louvain_communities.items()):\n",
    "    louvain_communities_dict.setdefault(value, []).append(key)\n",
    "\n",
    "print(len(louvain_communities_dict))\n",
    "print(louvain_communities_dict[0])\n",
    "print(sorted(louvain_communities_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning using Leiden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/leidenalg/\n",
    "https://www.nature.com/articles/s41598-019-41695-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categoriy of each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=('',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of categories of a page\n",
    "def get_categories(page_name):\n",
    "    c = list()\n",
    "    with driver.session() as session:\n",
    "        with session.begin_transaction() as tx:\n",
    "            for record in tx.run(\"MATCH (p:Page)-[:BELONGS_TO]->(c:Category) \"\n",
    "                                 \"WHERE p.title = {page_name} \"\n",
    "                                 \"AND NOT exists((c)-[:BELONGS_TO]->(:Category {title: \\'Hidden_categories\\'})) \"\n",
    "                                 \"RETURN c.title\", \n",
    "                                 page_name = page_name ):\n",
    "                #print(record[\"c.title\"])\n",
    "                c.append(record[\"c.title\"])\n",
    "    return c\n",
    "\n",
    "#map each element to frequency in a list    \n",
    "def count_frequency(my_list): \n",
    "      \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for items in my_list: \n",
    "        freq[items] = my_list.count(items)\n",
    "    return freq\n",
    "\n",
    "#iterate over pages dict partition\n",
    "def part_category_fetch(key, dic):\n",
    "    cat = []\n",
    "    for title in dic[key]:\n",
    "        cat += get_categories(title)\n",
    "    print('done fetching')\n",
    "    return cat\n",
    "\n",
    "def fetcher(bpd):\n",
    "    part_cat = {}\n",
    "    \n",
    "    for part in reversed(sorted(bpd)):\n",
    "        print(part)\n",
    "        cat = part_category_fetch(part, bpd)\n",
    "        print(cat)\n",
    "        part_cat.setdefault(part, cat)\n",
    "    \n",
    "    return part_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch categories for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_categories(bpd):\n",
    "    part_cat_dict = fetcher(bpd)\n",
    "    \n",
    "    return part_cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "part_cat_dict = fetch_categories(louvain_communities_dict)\n",
    "\n",
    "#with open('clusters to categoties.pickle', 'rb') as handle:\n",
    "#    d = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(part_cat_dict[len(part_cat_dict)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute frequency of category in each part and find max freq cat for each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_cat_dict_freq = {}\n",
    "for e in part_cat_dict:\n",
    "    cat_map_freq = count_frequency(part_cat_dict[e])\n",
    "    part_cat_dict_freq.setdefault(e, cat_map_freq)\n",
    "    \n",
    "    \n",
    "max_part_cat = {}\n",
    "for e in part_cat_dict_freq:\n",
    "    ls = list(part_cat_dict_freq[e].keys())\n",
    "    cat = ls[0]\n",
    "    for x in ls:\n",
    "        if part_cat_dict_freq[e][cat] < part_cat_dict_freq[e][x]:\n",
    "            cat = x\n",
    "    max_part_cat.setdefault(e, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in sorted(max_part_cat):\n",
    "    print(e, max_part_cat[e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe use TF-IDF between all possible categories for each cluster ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('LDA').config(\"spark.master\", \"local[*]\").config(\"spark.sql.warehouse.dir\", \"/home/ayman/warehouse\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(part_cat_dict[len(part_cat_dict) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = []\n",
    "for p in part_cat_dict:\n",
    "    df_.append((p, ' '.join(part_cat_dict[p]).replace('_', ' ').replace(',','').replace('\\\\\\'', ' ').lower()))\n",
    "        \n",
    "for e in df_:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionsData = spark.createDataFrame(df_, ['cluster', 'categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(partitionsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"categories\", outputCol=\"raw\")\n",
    "wordsDirtyData = tokenizer.transform(partitionsData)\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"words\")\n",
    "wordsData = remover.transform(wordsDirtyData)\n",
    "print(wordsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordsData.select('raw').first())\n",
    "print(wordsData.select('words').first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "#featurizedData = hashingTF.transform(wordsData)\n",
    "#print(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "cvmodel = countVectorizer.fit(wordsData)\n",
    "featurizedData = cvmodel.transform(wordsData)\n",
    "print(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "print(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = rescaledData.select('cluster', 'features')\n",
    "print(dataset)\n",
    "dataset.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trains a LDA model.\n",
    "lda = LDA(k=len(part_cat_dict), maxIter=100)\n",
    "ldaModel = lda.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ll = ldaModel.logLikelihood(dataset)\n",
    "lp = ldaModel.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = ldaModel.describeTopics(5)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show()\n",
    "\n",
    "# Shows the result\n",
    "transformed = ldaModel.transform(dataset)\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification for one cluster:\n",
    "getting the full row, getting the topic index and the corresponding topic informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = transformed.select('topicDistribution').first()[0]\n",
    "print(transformed.first())\n",
    "m = list(l).index(max(l))\n",
    "print('\\ntopic index is :',m)\n",
    "print(topics.take(m+1)[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the 5 most relevant words and their weights for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)\n",
    "vocabList = cvmodel.vocabulary\n",
    "tops = []\n",
    "for i,t,w in topicIndices.collect():\n",
    "    print('Topic %d:' % i)\n",
    "    entry = []\n",
    "    for j in range(len(t)):\n",
    "        entry.append(vocabList[t[j]])\n",
    "        #print('\\t', vocabList[t[j]], w[j])\n",
    "    print(entry)\n",
    "    tops.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/44233862/visualizing-topics-with-spark-lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing array of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign topic words to each cluster using LDA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_topicDist = sorted(transformed.select('cluster', 'topicDistribution').collect())\n",
    "cluster_topicTerms = []\n",
    "for e in cluster_topicDist:\n",
    "    m = list(e[1]).index(max(e[1]))\n",
    "    print(m)\n",
    "    cluster_topicTerms.append(tops[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DF with cluster number, its pages, the LDAtopics and the MAXcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_ = []\n",
    "for k in sorted(louvain_communities_dict.keys()):\n",
    "    df2_.append((k, louvain_communities_dict[k], cluster_topicTerms[k], max_part_cat[k]))\n",
    "    \n",
    "partitionsData2 = spark.createDataFrame(df2_, ['cluster', 'page names', 'LDAtopics', 'MAXcategory'])\n",
    "partitionsData2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
