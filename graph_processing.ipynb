{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('JAVA_HOME', '/usr/lib/jvm/java-1.8.0-openjdk-amd64')\n",
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import pickle\n",
    "import pyspark.ml\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from neo4j import GraphDatabase, Driver\n",
    "\n",
    "from pyspark.ml.feature import IDF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_graph(path):\n",
    "    print('importing graph from :', path)\n",
    "    directed_g = nx.read_gexf(path, node_type=None, relabel=True)\n",
    "    undirected_g = directed_g.to_undirected()\n",
    "    return undirected_g, directed_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_graph(g):\n",
    "    print('number of nodes : ' + str(len(g)))\n",
    "    fig, ax = plt.subplots(figsize=(70, 50)) # set size\n",
    "    nx.draw(g, with_labels=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_graph(graph):\n",
    "    print('cleaning graph')\n",
    "    #Degree computation\n",
    "    nodes_with_degrees = graph.degree\n",
    "    #mean\n",
    "    mean_deg = statistics.mean(l[1] for l in nodes_with_degrees)\n",
    "    #std\n",
    "    std_deg = statistics.stdev(l[1] for l in nodes_with_degrees)\n",
    "    #threshold\n",
    "    threshold = mean_deg + std_deg*std_deg/2 \n",
    "\n",
    "    #Filtering extremely highly connected nodes\n",
    "    nodes_to_remove = list(filter(lambda d : d[1] > threshold, nodes_with_degrees))\n",
    "    n,d = zip(*nodes_to_remove)\n",
    "    graph.remove_nodes_from(n)\n",
    "    \n",
    "    #Filtering isolated nodes\n",
    "    nodes_with_degrees = graph.degree\n",
    "    nodes_to_remove = list(filter(lambda d : d[1] == 0, nodes_with_degrees))\n",
    "    n,d = zip(*nodes_to_remove)\n",
    "    graph.remove_nodes_from(n)\n",
    "\n",
    "    print('remaining nodes : ' + str(len(graph)))\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only largest connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_connected_component(graph):\n",
    "    print('largest connected component')\n",
    "    gcc = max(nx.connected_component_subgraphs(graph), key=len)\n",
    "    print('remaining nodes : ' + str(len(gcc)))\n",
    "    return gcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning using Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communities_louvain(graph):\n",
    "    louvain_communities = community.best_partition(graph, resolution=1)\n",
    "    louvain_communities_dict = {}\n",
    "    for key, value in sorted(louvain_communities.items()):\n",
    "        louvain_communities_dict.setdefault(value, []).append(key)\n",
    "\n",
    "    print('detcted',len(louvain_communities_dict),'communities')\n",
    "    \n",
    "    return louvain_communities_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning using Leiden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/leidenalg/\n",
    "https://www.nature.com/articles/s41598-019-41695-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categoriy of each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=('neo4j','tototo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of categories of a page\n",
    "def get_categories(page_name):\n",
    "    c = list()\n",
    "    with driver.session() as session:\n",
    "        with session.begin_transaction() as tx:\n",
    "            for record in tx.run(\"MATCH (p:Page)-[:BELONGS_TO]->(c:Category) \"\n",
    "                                 \"WHERE p.title = {page_name} \"\n",
    "                                 \"AND NOT exists((c)-[:BELONGS_TO]->(:Category {title: \\'Hidden_categories\\'})) \"\n",
    "                                 \"RETURN c.title\", \n",
    "                                 page_name = page_name ):\n",
    "                #print(record[\"c.title\"])\n",
    "                c.append(record[\"c.title\"])\n",
    "    return c\n",
    "\n",
    "#map each element to frequency in a list    \n",
    "def count_frequency(my_list): \n",
    "      \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for items in my_list: \n",
    "        freq[items] = my_list.count(items)\n",
    "    return freq\n",
    "\n",
    "#iterate over pages dict partition\n",
    "def part_category_fetch(key, dic):\n",
    "    cat = []\n",
    "    for title in dic[key]:\n",
    "        cat += get_categories(title)\n",
    "    #print('done fetching')\n",
    "    return cat\n",
    "\n",
    "def fetcher(bpd):\n",
    "    part_cat = {}\n",
    "    \n",
    "    for part in sorted(bpd):\n",
    "        #print(part)\n",
    "        cat = part_category_fetch(part, bpd)\n",
    "        #print(cat)\n",
    "        part_cat.setdefault(part, cat)\n",
    "    \n",
    "    return part_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch categories for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_categories(bpd):\n",
    "    part_cat_dict = fetcher(bpd)\n",
    "    \n",
    "    return part_cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_frequencies(d):\n",
    "    part_cat_dict_freq = {}\n",
    "    for e in d:\n",
    "        cat_map_freq = count_frequency(d[e])\n",
    "        part_cat_dict_freq.setdefault(e, cat_map_freq)\n",
    "    return part_cat_dict_freq\n",
    " \n",
    "def find_max_freq(p):\n",
    "    max_part_cat = {}\n",
    "    for e in p:\n",
    "        ls = list(p[e].keys())\n",
    "        cat = ls[0]\n",
    "        for x in ls:\n",
    "            if p[e][cat] < p[e][x]:\n",
    "                cat = x\n",
    "        max_part_cat.setdefault(e, cat)\n",
    "    return max_part_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_largest_communities(c, n):\n",
    "    x = min(n, len(c))\n",
    "    print('getting', x, 'largest communities')\n",
    "    \n",
    "    l = []\n",
    "    for e in sorted(c):\n",
    "        l.append(c[e])\n",
    "    \n",
    "    tmp = sorted(l, key=len, reverse=True)[:x]\n",
    "    \n",
    "    res = {}\n",
    "    for i in range(x):\n",
    "        res.setdefault(i, tmp[i])\n",
    "    \n",
    "    length = 0\n",
    "    for e in res:\n",
    "        length += len(res[e])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ld(path, n):\n",
    "    undirected_g, directed_g = import_graph(path)\n",
    "    #verify_graph(g)\n",
    "    gg = largest_connected_component(clean_graph(undirected_g))\n",
    "    #verify_graph(gg)\n",
    "    communities_ = communities_louvain(gg)\n",
    "    communities = get_n_largest_communities(communities_, n)\n",
    "    undirected_graph = gg.subgraph([x for y in communities.values() for x in y])\n",
    "    directed_graph = directed_g.subgraph([x for y in communities.values() for x in y])\n",
    "    print('new number of nodes is :', len(directed_graph))\n",
    "    part_cat_dict = fetch_categories(communities)\n",
    "    return undirected_graph, directed_graph, communities, part_cat_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    print('tokenizing')\n",
    "    tokenizer = Tokenizer(inputCol=\"categories\", outputCol=\"raw\")\n",
    "    res = tokenizer.transform(df)\n",
    "    return res\n",
    "    \n",
    "def stop_words_remove(df):\n",
    "    print('stopWords removal')\n",
    "    remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"words\")\n",
    "    res = remover.transform(df)\n",
    "    return res\n",
    "\n",
    "def lemmatize(df):\n",
    "    print('lemmatization')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_udf = udf(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens], ArrayType(StringType()))\n",
    "    res = df.withColumn(\"words\", lemmatizer_udf(\"words\"))\n",
    "    return res\n",
    "\n",
    "def stem(df):\n",
    "    print('stemming')\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "    res = df.withColumn(\"words\", stemmer_udf(\"words\"))\n",
    "    return res\n",
    "\n",
    "def cv_fit(df) :   \n",
    "    print('countVectorizer')\n",
    "    countVectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "    cvmodel = countVectorizer.fit(df)\n",
    "    return cvmodel\n",
    "\n",
    "def cv_transform(cvmodel, df):\n",
    "    res = cvmodel.transform(df)\n",
    "    return res\n",
    "\n",
    "def idf(df):\n",
    "    print('IDF')\n",
    "    idf_ = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf_.fit(df)\n",
    "    res = idfModel.transform(df)\n",
    "    return res\n",
    "    \n",
    "    #dataset = rescaledData.select('cluster','categories', 'features')\n",
    "    #print(dataset)\n",
    "\n",
    "    #dataset.show(truncate=False)\n",
    "    \n",
    "def lda_fit(df):\n",
    "    # Trains a LDA model.\n",
    "    print('training LDA')\n",
    "    lda_ = LDA(k=df.count(), maxIter=50)\n",
    "    ldaModel = lda_.fit(df)\n",
    "    return ldaModel\n",
    "\n",
    "def lda_transform(ldaModel, df):\n",
    "    print('LDA transformation')\n",
    "    transformed = ldaModel.transform(df)\n",
    "    #transformed.show()\n",
    "    return transformed\n",
    "    \n",
    "#    l = transformed.select('topicDistribution').first()[0]\n",
    "#    print(transformed.first())\n",
    "#    m = list(l).index(max(l))\n",
    "#    print('\\ntopic index is :',m)\n",
    "#    print(topics.take(m+1)[m])\n",
    "    \n",
    "def show_topic_description(ldaModel, cvmodel):\n",
    "    topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)\n",
    "    vocabList = cvmodel.vocabulary\n",
    "    tops = []\n",
    "    for i,t,w in topicIndices.collect():\n",
    "        print('Topic %d:' % i)\n",
    "        entry = []\n",
    "        for j in range(len(t)):\n",
    "            entry.append(vocabList[t[j]])\n",
    "            #print('\\t', vocabList[t[j]], w[j])\n",
    "        print(entry)\n",
    "        tops.append(entry)\n",
    "        \n",
    "    return tops\n",
    "    \n",
    "def get_topics(spark, communities, tops, transformed):\n",
    "    cluster_topicDist = sorted(transformed.select('cluster', 'topicDistribution').collect())\n",
    "    cluster_topicTerms = []\n",
    "    for e in cluster_topicDist:\n",
    "        m = list(e[1]).index(max(e[1]))\n",
    "        cluster_topicTerms.append(tops[m])\n",
    "\n",
    "    df2_ = []\n",
    "    for k in sorted(communities.keys()):\n",
    "        df2_.append((k, ' - '.join(communities[k]), ' - '.join(cluster_topicTerms[k])))\n",
    "    \n",
    "    partitionsData2 = spark.createDataFrame(df2_, ['cluster', 'page names', 'LDA topics'])\n",
    "    partitionsData2.select('cluster', 'LDA topics').show(truncate=False)\n",
    "    return partitionsData2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def topics_with_ml(spark, communities, part_cat_dict):\n",
    "    df_ = []\n",
    "    for p in part_cat_dict:\n",
    "        df_.append((p, ' '.join(part_cat_dict[p]).replace('_', ' ').replace(',', '')\\\n",
    "                    .replace('\\\\\\'', ' ').replace('(', '').replace(')', '').lower()))\n",
    "\n",
    "    partitionsData = spark.createDataFrame(df_, ['cluster', 'categories'])\n",
    "\n",
    "    tokenized = tokenize(partitionsData)\n",
    "    cleaned = stop_words_remove(tokenized)\n",
    "    lemmatized = lemmatize(cleaned)\n",
    "    #stemmed = stem(lemmatized)\n",
    "    #cvModel = cv_fit(stemmed)\n",
    "    cvModel = cv_fit(lemmatized)\n",
    "    #cv = cv_transform(cvModel, stemmed)\n",
    "    cv = cv_transform(cvModel, lemmatized)\n",
    "    rescaled = idf(cv)\n",
    "    ldaModel = lda_fit(rescaled)\n",
    "    ldaTransformed = lda_transform(ldaModel, rescaled)\n",
    "    tops = show_topic_description(ldaModel, cvModel)\n",
    "    final = get_topics(spark, communities, tops, ldaTransformed)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betweenness_centrality_nodes(graph, clusters_dict):\n",
    "    res = {}\n",
    "    for e in clusters_dict:\n",
    "        H = graph.subgraph(clusters_dict[e])\n",
    "        d = nx.algorithms.centrality.betweenness_centrality_subset(H, H.nodes, H.nodes)\n",
    "        \n",
    "        m = 0\n",
    "        n = None\n",
    "        for i in d:\n",
    "            if d[i] > m:\n",
    "                m = d[i]\n",
    "                n = i\n",
    "        res.setdefault(e, n)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pagerank(dir_graph, clusters_dict):\n",
    "    pr = nx.algorithms.link_analysis.pagerank_alg.pagerank(dir_graph)\n",
    "    res = {}\n",
    "    for i in clusters_dict:\n",
    "        m = 0\n",
    "        n = None\n",
    "        for p in clusters_dict[i]:\n",
    "            if pr[p] > m:\n",
    "                m = pr[p]\n",
    "                n = p\n",
    "        res.setdefault(i, n)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pagerank_on_clusters(dir_graph, clusters_dict):\n",
    "    res = {}\n",
    "    for e in clusters_dict:\n",
    "        H = dir_graph.subgraph(clusters_dict[e])\n",
    "        pr = nx.algorithms.link_analysis.pagerank_alg.pagerank(H)\n",
    "        m = 0\n",
    "        n = None\n",
    "        for i in clusters_dict[e]:\n",
    "            if pr[i] > m:\n",
    "                m = pr[i]\n",
    "                n = i\n",
    "        res.setdefault(e, n)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_degree(graph, clusters_dict):\n",
    "    d = graph.degree\n",
    "    res = {}\n",
    "    for i in clusters_dict:\n",
    "        m = 0\n",
    "        n = None\n",
    "        for p in clusters_dict[i]:\n",
    "            if d[p] > m:\n",
    "                m = d[p]\n",
    "                n = p\n",
    "        res.setdefault(i, n)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_degree_on_clusters(graph, clusters_dict):\n",
    "    res = {}\n",
    "    for e in clusters_dict:\n",
    "        H = graph.subgraph(clusters_dict[e])\n",
    "        d = H.degree\n",
    "        m = 0\n",
    "        n = None\n",
    "        for p in clusters_dict[e]:\n",
    "            if d[p] > m:\n",
    "                m = d[p]\n",
    "                n = p\n",
    "        res.setdefault(e, n)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(df, m, c, pr, pr2, d, d2):\n",
    "    def get_maxx(i):\n",
    "        return m[i]\n",
    "    udf_get_maxx = udf(get_maxx, StringType())\n",
    "\n",
    "    def get_central(i):\n",
    "        return c[i]\n",
    "    udf_get_central = udf(get_central, StringType())\n",
    "    \n",
    "    def get_pr(i):\n",
    "        return pr[i]\n",
    "    udf_get_pr = udf(get_pr, StringType())\n",
    "    \n",
    "    def get_pr2(i):\n",
    "        return pr2[i]\n",
    "    udf_get_pr2 = udf(get_pr2, StringType())\n",
    "    \n",
    "    def get_d(i):\n",
    "        return d[i]\n",
    "    udf_get_d = udf(get_d, StringType())\n",
    "    \n",
    "    def get_d2(i):\n",
    "        return d2[i]\n",
    "    udf_get_d2 = udf(get_d2, StringType())\n",
    "\n",
    "    a = df.withColumn('betweenness central node', udf_get_central('cluster'))\\\n",
    "    .withColumn('max pagerank', udf_get_pr('cluster'))\\\n",
    "    .withColumn('max isolated pagerank', udf_get_pr2('cluster'))\\\n",
    "    .withColumn('max degree', udf_get_d('cluster'))\\\n",
    "    .withColumn('max isolated degree', udf_get_d2('cluster'))\\\n",
    "    .withColumn('max category', udf_get_maxx('cluster'))\n",
    "    \n",
    "    #a.show()\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "mypath = 'graphs/'\n",
    "max_num_communities = 20\n",
    "output_path = 'output/'\n",
    "(_, _, filenames) = next(walk(mypath))\n",
    "#filenames = [\"peaks_graph_20190901_20190915.gexf\"]\n",
    "\n",
    "spark = SparkSession.builder.appName('LDA').config(\"spark.master\", \"local[*]\").config(\"spark.sql.warehouse.dir\", \"/home/ayman/warehouse\").getOrCreate()\n",
    "\n",
    "for f in sorted(filenames):\n",
    "    path = mypath + f\n",
    "    (G_undir, G_dir, communities, part_cat_dict) = ld(path, max_num_communities)\n",
    "    maxx = find_max_freq(count_all_frequencies(part_cat_dict))\n",
    "    lda_df = topics_with_ml(spark, communities, part_cat_dict)\n",
    "    betweenness_central_nodes = betweenness_centrality_nodes(G_undir, communities)\n",
    "    pr_result = max_pagerank(G_dir, communities)\n",
    "    pr2_result = max_pagerank_on_clusters(G_dir, communities)\n",
    "    deg = max_degree(G_undir, communities)\n",
    "    deg2 = max_degree_on_clusters(G_undir, communities)\n",
    "    res = merge(lda_df, maxx, betweenness_central_nodes, pr_result, pr2_result, deg, deg2)\n",
    "    print('betweenness central nodes visualization')\n",
    "    res.select('cluster', 'LDA topics', 'betweenness central node').show(truncate=False)\n",
    "    print('max pagerank visualization')\n",
    "    res.select('cluster', 'LDA topics', 'max pagerank').show(truncate=False)\n",
    "    print('max pagerank2 visualization')\n",
    "    res.select('cluster', 'LDA topics', 'max isolated pagerank').show(truncate=False)\n",
    "    print('max deg visualization')\n",
    "    res.select('cluster', 'LDA topics', 'max degree').show(truncate=False)\n",
    "    print('max deg2 visualization')\n",
    "    res.select('cluster', 'LDA topics', 'max isolated degree').show(truncate=False)\n",
    "    res.coalesce(1).write.csv(output_path+f[12:-5], mode = 'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''print('betweenness central nodes visualization')\n",
    "res.select('cluster', 'LDA topics', 'betweenness central node').show(truncate=False)\n",
    "print('max pagerank visualization')\n",
    "res.select('cluster', 'LDA topics', 'max pagerank').show(truncate=False)\n",
    "print('max pagerank2 visualization')\n",
    "res.select('cluster', 'LDA topics', 'max isolated pagerank').show(truncate=False)\n",
    "print('max deg visualization')\n",
    "res.select('cluster', 'LDA topics', 'max degree').show(truncate=False)\n",
    "print('max deg2 visualization')\n",
    "res.select('cluster', 'LDA topics', 'max isolated degree').show(truncate=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''import argparse\n",
    "\n",
    "def parseArguments():\n",
    "    # Create argument parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Optional arguments\n",
    "    parser.add_argument(\"-ip\", \"--inputPath\", help=\"path of the directory containing the graphs.\", type=str, default='graphs/')\n",
    "    parser.add_argument(\"-n\", \"--nOfClusters\", help=\"max number of clusters to extract.\", type=int, default=20)\n",
    "    parser.add_argument(\"-op\", \"--outputPath\", help=\"path of the output directory.\", type=str, default='output/')\n",
    "\n",
    "    # Parse arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "    \n",
    "main(parseArguments())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more centrality attributes: pagerank, degrees, and try to find others\n",
    "\n",
    "make code a sort of an executable tool with arguments etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take node with highest degree in each cluster and find its neighbors and seee how many are in the same cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
