{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import community\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "from neo4j import GraphDatabase, Driver\n",
    "\n",
    "import pyspark.ml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import IDF, Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_graph(path):\n",
    "    print('importing graph from :', path)\n",
    "    directed_g = nx.read_gexf(path, node_type=None, relabel=True)\n",
    "    undirected_g = directed_g.to_undirected()\n",
    "    return undirected_g, directed_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_graph(g):\n",
    "    print('number of nodes : ' + str(len(g)))\n",
    "    fig, ax = plt.subplots(figsize=(70, 50)) # set size\n",
    "    nx.draw(g, with_labels=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_graph(graph):\n",
    "    print('cleaning graph')\n",
    "    #Degree computation\n",
    "    nodes_with_degrees = graph.degree\n",
    "    #mean\n",
    "    mean_deg = statistics.mean(l[1] for l in nodes_with_degrees)\n",
    "    #std\n",
    "    std_deg = statistics.stdev(l[1] for l in nodes_with_degrees)\n",
    "    #threshold\n",
    "    threshold = mean_deg + std_deg*std_deg/2 \n",
    "\n",
    "    #Filtering extremely highly connected nodes\n",
    "    nodes_to_remove = list(filter(lambda d : d[1] > threshold, nodes_with_degrees))\n",
    "    n,d = zip(*nodes_to_remove)\n",
    "    graph.remove_nodes_from(n)\n",
    "    \n",
    "    #Filtering isolated nodes\n",
    "    nodes_with_degrees = graph.degree\n",
    "    nodes_to_remove = list(filter(lambda d : d[1] == 0, nodes_with_degrees))\n",
    "    n,d = zip(*nodes_to_remove)\n",
    "    graph.remove_nodes_from(n)\n",
    "\n",
    "    print('remaining nodes : ' + str(len(graph)))\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only largest connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_connected_component(graph):\n",
    "    print('largest connected component')\n",
    "    gcc = max(nx.connected_component_subgraphs(graph), key=len)\n",
    "    print('remaining nodes : ' + str(len(gcc)))\n",
    "    return gcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning using Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communities_louvain(graph):\n",
    "    louvain_communities = community.best_partition(graph, resolution=1)\n",
    "    louvain_communities_dict = {}\n",
    "    for key, value in sorted(louvain_communities.items()):\n",
    "        louvain_communities_dict.setdefault(value, []).append(key)\n",
    "\n",
    "    print('detcted',len(louvain_communities_dict),'communities')\n",
    "    \n",
    "    return louvain_communities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning using Leiden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/leidenalg/\n",
    "https://www.nature.com/articles/s41598-019-41695-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leiden helpers/adapters\n",
    "def get_id_to_title(graph):\n",
    "    tmp = nx.get_node_attributes(graph, 'id')\n",
    "    d = {}\n",
    "    for x in tmp:\n",
    "        d.setdefault(tmp[x], x)\n",
    "    return d\n",
    "\n",
    "def nx_to_ig(graph):\n",
    "    nx.write_graphml(graph,'graph.graphml')\n",
    "    graphi = ig.read('graph.graphml',format=\"graphml\")\n",
    "    os.remove('graph.graphml')\n",
    "    return graphi\n",
    "\n",
    "def translate_leiden_to_dict(partition, graphi, dictionary):\n",
    "    nodes = graphi.vs\n",
    "    res = {}\n",
    "    for i in range(len(partition)):\n",
    "        res.setdefault(i, [])\n",
    "        for v in partition[i]:\n",
    "            res[i].append(dictionary[nodes[v]['id']])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communities_leiden(graph):\n",
    "    graphi = nx_to_ig(graph)\n",
    "    partition = la.find_partition(graphi, la.ModularityVertexPartition)\n",
    "    dictionary = get_id_to_title(graph)\n",
    "    res = translate_leiden_to_dict(partition, graphi, dictionary)\n",
    "    \n",
    "    print('detcted',len(res),'communities')\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categoriy of each partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of categories of a page\n",
    "def get_categories(page_name):\n",
    "    c = list()\n",
    "    with driver.session() as session:\n",
    "        with session.begin_transaction() as tx:\n",
    "            for record in tx.run(\"MATCH (p:Page)-[:BELONGS_TO]->(c:Category) \"\n",
    "                                 \"WHERE p.title = {page_name} \"\n",
    "                                 \"AND NOT exists((c)-[:BELONGS_TO]->(:Category {title: \\'{hc}\\'})) \"\n",
    "                                 \"RETURN c.title\", \n",
    "                                 page_name = page_name,\n",
    "                                 hc = language_mapper[language]):\n",
    "                #print(record[\"c.title\"])\n",
    "                c.append(record[\"c.title\"])\n",
    "    return c\n",
    "\n",
    "#map each element to frequency in a list    \n",
    "def count_frequency(my_list): \n",
    "      \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for items in my_list: \n",
    "        freq[items] = my_list.count(items)\n",
    "    return freq\n",
    "\n",
    "#iterate over pages dict partition\n",
    "def part_category_fetch(key, dic):\n",
    "    cat = []\n",
    "    for title in dic[key]:\n",
    "        cat += get_categories(title)\n",
    "    #print('done fetching')\n",
    "    return cat\n",
    "\n",
    "def fetcher(bpd):\n",
    "    part_cat = {}\n",
    "    \n",
    "    for part in sorted(bpd):\n",
    "        #print(part)\n",
    "        cat = part_category_fetch(part, bpd)\n",
    "        #print(cat)\n",
    "        part_cat.setdefault(part, cat)\n",
    "    \n",
    "    return part_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch categories for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_categories(bpd):\n",
    "    part_cat_dict = fetcher(bpd)\n",
    "    \n",
    "    return part_cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_frequencies(d):\n",
    "    part_cat_dict_freq = {}\n",
    "    for e in d:\n",
    "        cat_map_freq = count_frequency(d[e])\n",
    "        part_cat_dict_freq.setdefault(e, cat_map_freq)\n",
    "    return part_cat_dict_freq\n",
    " \n",
    "def find_max_freq(p):\n",
    "    max_part_cat = {}\n",
    "    for e in p:\n",
    "        ls = list(p[e].keys())\n",
    "        cat = ls[0]\n",
    "        for x in ls:\n",
    "            if p[e][cat] < p[e][x]:\n",
    "                cat = x\n",
    "        max_part_cat.setdefault(e, cat)\n",
    "    return max_part_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_largest_communities(c, n):\n",
    "    x = min(n, len(c))\n",
    "    print('getting', x, 'largest communities')\n",
    "    \n",
    "    l = []\n",
    "    for e in sorted(c):\n",
    "        l.append(c[e])\n",
    "    \n",
    "    tmp = sorted(l, key=len, reverse=True)[:x]\n",
    "    \n",
    "    res = {}\n",
    "    for i in range(x):\n",
    "        res.setdefault(i, tmp[i])\n",
    "    \n",
    "    length = 0\n",
    "    for e in res:\n",
    "        length += len(res[e])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ld(path, n):\n",
    "    undirected_g, directed_g = import_graph(path)\n",
    "    #verify_graph(g)\n",
    "    gg = largest_connected_component(clean_graph(undirected_g))\n",
    "    #verify_graph(gg)\n",
    "    \n",
    "    #communities_dict = communities_louvain(gg)\n",
    "    communities_dict = communities_leiden(gg)\n",
    "    \n",
    "    communities = get_n_largest_communities(communities_dict, n)\n",
    "    \n",
    "    undirected_graph = gg.subgraph([x for y in communities.values() for x in y])\n",
    "    directed_graph = directed_g.subgraph([x for y in communities.values() for x in y])\n",
    "        \n",
    "    print('new number of nodes is :', len(directed_graph))\n",
    "    \n",
    "    part_cat_dict = fetch_categories(communities)\n",
    "    \n",
    "    return undirected_graph, directed_graph, communities, part_cat_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    print('tokenizing')\n",
    "    tokenizer = Tokenizer(inputCol=\"categories\", outputCol=\"raw\")\n",
    "    res = tokenizer.transform(df)\n",
    "    return res\n",
    "    \n",
    "def stop_words_remove(df):\n",
    "    print('stopWords removal')\n",
    "    remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"words\")\n",
    "    res = remover.transform(df)\n",
    "    return res\n",
    "\n",
    "def lemmatize(df):\n",
    "    print('lemmatization')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_udf = udf(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens], T.ArrayType(T.StringType()))\n",
    "    res = df.withColumn(\"words\", lemmatizer_udf(\"words\"))\n",
    "    return res\n",
    "\n",
    "def lemmatize_new(df):\n",
    "    #nlp = spacy.load(language_mapper[language]['lemmatizer'])\n",
    "    print('new lemmatization')\n",
    "\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "    lemmatizer_udf = udf(lambda tokens: [' '.join([w.lemma_ for w in nlp(token)]) for token in tokens], T.ArrayType(T.StringType()))\n",
    "    res = df.withColumn(\"words\", lemmatizer_udf(\"words\"))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def stem(df):\n",
    "    print('stemming')\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], T.ArrayType(T.StringType()))\n",
    "    res = df.withColumn(\"words\", stemmer_udf(\"words\"))\n",
    "    return res\n",
    "\n",
    "def cv_fit(df):\n",
    "    print('countVectorizer')\n",
    "    countVectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "    cvmodel = countVectorizer.fit(df)\n",
    "    return cvmodel\n",
    "\n",
    "def cv_transform(cvmodel, df):\n",
    "    res = cvmodel.transform(df)\n",
    "    return res\n",
    "\n",
    "def idf(df):\n",
    "    print('IDF')\n",
    "    idf_ = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf_.fit(df)\n",
    "    res = idfModel.transform(df)\n",
    "    return res\n",
    "    \n",
    "    #dataset = rescaledData.select('cluster','categories', 'features')\n",
    "    #print(dataset)\n",
    "\n",
    "    #dataset.show(truncate=False)\n",
    "    \n",
    "def lda_fit(df):\n",
    "    # Trains a LDA model.\n",
    "    print('training LDA')\n",
    "    lda_ = LDA(k=df.count(), maxIter=100)\n",
    "    ldaModel = lda_.fit(df)\n",
    "    return ldaModel\n",
    "\n",
    "def lda_transform(ldaModel, df):\n",
    "    print('LDA transformation')\n",
    "    transformed = ldaModel.transform(df)\n",
    "    #transformed.show()\n",
    "    return transformed\n",
    "    \n",
    "#    l = transformed.select('topicDistribution').first()[0]\n",
    "#    print(transformed.first())\n",
    "#    m = list(l).index(max(l))\n",
    "#    print('\\ntopic index is :',m)\n",
    "#    print(topics.take(m+1)[m])\n",
    "    \n",
    "def show_topic_description(ldaModel, cvmodel):\n",
    "    topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)\n",
    "    vocabList = cvmodel.vocabulary\n",
    "    tops = []\n",
    "    for i,t,w in topicIndices.collect():\n",
    "        print('Topic %d:' % i)\n",
    "        entry = []\n",
    "        for j in range(len(t)):\n",
    "            entry.append(vocabList[t[j]])\n",
    "            #print('\\t', vocabList[t[j]], w[j])\n",
    "        print(entry)\n",
    "        tops.append(entry)\n",
    "        \n",
    "    return tops\n",
    "    \n",
    "def get_topics(communities, tops, transformed):\n",
    "    cluster_topicDist = sorted(transformed.select('cluster', 'topicDistribution').collect())\n",
    "    cluster_topicTerms = []\n",
    "    for e in cluster_topicDist:\n",
    "        m = list(e[1]).index(max(e[1]))\n",
    "        cluster_topicTerms.append(tops[m])\n",
    "\n",
    "    df2_ = []\n",
    "    for k in sorted(communities.keys()):\n",
    "        df2_.append((k, ' - '.join(communities[k]), ' - '.join(cluster_topicTerms[k])))\n",
    "    \n",
    "    partitionsData2 = spark.createDataFrame(df2_, ['cluster', 'page names', 'LDA topics'])\n",
    "    partitionsData2.select('cluster', 'LDA topics').show(truncate=False)\n",
    "    return partitionsData2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(part_cat_dict):\n",
    "    df_ = []\n",
    "    for p in part_cat_dict:\n",
    "        df_.append((p, ' '.join(part_cat_dict[p]).replace('_', ' ').replace(',', '')\\\n",
    "                    .replace('\\\\\\'', ' ').replace('(', '').replace(')', '').replace('–',' ').lower()))\n",
    "\n",
    "    partitionsData = spark.createDataFrame(df_, ['cluster', 'categories'])\n",
    "    return partitionsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def topics_with_ml(communities, part_cat_dict):\n",
    "    \n",
    "    partitionsData = create_df(part_cat_dict)\n",
    "    tokenized = tokenize(partitionsData)\n",
    "    cleaned = stop_words_remove(tokenized)\n",
    "    \n",
    "    if (language == 'en'):\n",
    "        lemmatized = lemmatize(cleaned)\n",
    "    else:\n",
    "        lemmatized = lemmatize_new(cleaned)\n",
    "    \n",
    "    #stemmed = stem(lemmatized)\n",
    "    #cvModel = cv_fit(stemmed)\n",
    "    cvModel = cv_fit(lemmatized)\n",
    "    #cv = cv_transform(cvModel, stemmed)\n",
    "    cv = cv_transform(cvModel, lemmatized)\n",
    "    rescaled = idf(cv)\n",
    "    ldaModel = lda_fit(rescaled)\n",
    "    ldaTransformed = lda_transform(ldaModel, rescaled)\n",
    "    tops = show_topic_description(ldaModel, cvModel)\n",
    "    final = get_topics(communities, tops, ldaTransformed)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betweenness_centrality_nodes(graph, clusters_dict):\n",
    "    res = {}\n",
    "    for e in clusters_dict:\n",
    "        H = graph.subgraph(clusters_dict[e])\n",
    "        d = nx.algorithms.centrality.betweenness_centrality_subset(H, H.nodes, H.nodes)\n",
    "        \n",
    "        m = 0\n",
    "        n = None\n",
    "        for i in d:\n",
    "            if d[i] > m:\n",
    "                m = d[i]\n",
    "                n = i\n",
    "        res.setdefault(e, n)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pagerank(dir_graph, clusters_dict):\n",
    "    pr = nx.algorithms.link_analysis.pagerank_alg.pagerank(dir_graph)\n",
    "    res = {}\n",
    "    for i in clusters_dict:\n",
    "        m = 0\n",
    "        n = None\n",
    "        for p in clusters_dict[i]:\n",
    "            if pr[p] > m:\n",
    "                m = pr[p]\n",
    "                n = p\n",
    "        res.setdefault(i, n)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pagerank_on_clusters(dir_graph, clusters_dict):\n",
    "    res = {}\n",
    "    for e in clusters_dict:\n",
    "        H = dir_graph.subgraph(clusters_dict[e])\n",
    "        pr = nx.algorithms.link_analysis.pagerank_alg.pagerank(H)\n",
    "        m = 0\n",
    "        n = None\n",
    "        for i in clusters_dict[e]:\n",
    "            if pr[i] > m:\n",
    "                m = pr[i]\n",
    "                n = i\n",
    "        res.setdefault(e, n)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_degree(graph, clusters_dict):\n",
    "    d = graph.degree\n",
    "    res = {}\n",
    "    for i in clusters_dict:\n",
    "        m = 0\n",
    "        n = None\n",
    "        for p in clusters_dict[i]:\n",
    "            if d[p] > m:\n",
    "                m = d[p]\n",
    "                n = p\n",
    "        res.setdefault(i, n)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_degree_on_clusters(graph, clusters_dict):\n",
    "    res = {}\n",
    "    for e in clusters_dict:\n",
    "        H = graph.subgraph(clusters_dict[e])\n",
    "        d = H.degree\n",
    "        m = 0\n",
    "        n = None\n",
    "        for p in clusters_dict[e]:\n",
    "            if d[p] > m:\n",
    "                m = d[p]\n",
    "                n = p\n",
    "        res.setdefault(e, n)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column(df, c, name:str, typ):\n",
    "    def get_el(i):\n",
    "        return c[i]\n",
    "    \n",
    "    udf_get_el = udf(get_el, typ)\n",
    "    \n",
    "    a = df.withColumn(name, udf_get_el('cluster'))\n",
    "    \n",
    "    #a.show()\n",
    "    \n",
    "    return a\n",
    "\n",
    "def add_columns(df, l):\n",
    "    r = df\n",
    "    for col, tag in l:\n",
    "        r = add_column(r, col, tag, T.StringType())\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attributes_from_df(graph, df):\n",
    "    rowList = df.collect()\n",
    "    d = {}\n",
    "    for row in rowList:\n",
    "        for p in row[1].split(\" - \"):\n",
    "            d.setdefault(p, {\"community\": row[0], \"lda\": row[2]})\n",
    "            \n",
    "    nx.set_node_attributes(graph, d)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ayman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access UI on : http://0.0.0.0:4040\n",
      "Spark warehouse set to : /home/ayman/warehouse\n",
      "importing graph from : graphs/fr/peaks_graph_20180901_20180915.gexf\n",
      "cleaning graph\n",
      "remaining nodes : 927\n",
      "largest connected component\n",
      "remaining nodes : 671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/igraph/__init__.py:2223: RuntimeWarning: Could not add vertex ids, there is already an 'id' vertex attribute at foreign-graphml.c:443\n",
      "  return reader(f, *args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detcted 26 communities\n",
      "getting 20 largest communities\n",
      "new number of nodes is : 615\n",
      "tokenizing\n",
      "stopWords removal\n",
      "new lemmatization\n",
      "countVectorizer\n",
      "IDF\n",
      "training LDA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-be8eda18d521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmaxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_max_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_all_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart_cat_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mlda_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopics_with_ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpart_cat_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mbetweenness_central_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetweenness_centrality_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_undir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f1ffb56748c7>\u001b[0m in \u001b[0;36mtopics_with_ml\u001b[0;34m(communities, part_cat_dict)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrescaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mldaModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mldaTransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_topic_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-605c5a578010>\u001b[0m in \u001b[0;36mlda_fit\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training LDA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mlda_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mldaModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mldaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ.setdefault('JAVA_HOME', '/usr/lib/jvm/java-1.8.0-openjdk-amd64')\n",
    "user = os.environ.get('USER')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "global lanuage\n",
    "language = 'fr'\n",
    "\n",
    "from os import walk\n",
    "\n",
    "mypath = 'graphs/'+language+'/'\n",
    "max_num_communities = 20\n",
    "output_path = 'output'+language+'/'\n",
    "#(_, _, filenames) = next(walk(mypath))\n",
    "#filenames = [\"peaks_graph_20190901_20190915.gexf\"]\n",
    "filenames = [\"peaks_graph_20180901_20180915.gexf\"]\n",
    "\n",
    "\n",
    "global language_mapper\n",
    "language_mapper = {\n",
    "    'en': 'Hidden_categories',\n",
    "    'fr': 'Catégorie_cachée'\n",
    "}\n",
    "\n",
    "global driver\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=('neo4j', 'tototo'))\n",
    "\n",
    "global spark\n",
    "spark = SparkSession.builder.appName('graph processing').config(\"spark.master\", \"local[*]\").config(\"spark.sql.warehouse.dir\", \"/home/\"+user+\"/warehouse\").getOrCreate()\n",
    "print(\"Access UI on : http://0.0.0.0:\" + spark.sparkContext.uiWebUrl.split(\":\")[-1])\n",
    "print(\"Spark warehouse set to :\", spark.conf.get('spark.sql.warehouse.dir'))\n",
    "\n",
    "for f in sorted(filenames):\n",
    "    path = mypath + f\n",
    "    (G_undir, G_dir, communities, part_cat_dict) = ld(path, max_num_communities)\n",
    "        \n",
    "    maxx = find_max_freq(count_all_frequencies(part_cat_dict))\n",
    "    \n",
    "    lda_df = topics_with_ml(communities, part_cat_dict)\n",
    "    \n",
    "    betweenness_central_nodes = betweenness_centrality_nodes(G_undir, communities)\n",
    "    \n",
    "    pr_result = max_pagerank(G_dir, communities)\n",
    "    pr_iso_result = max_pagerank_on_clusters(G_dir, communities)\n",
    "    \n",
    "    deg_result = max_degree(G_undir, communities)\n",
    "    deg_iso_result = max_degree_on_clusters(G_undir, communities)\n",
    "    \n",
    "    res = add_columns(lda_df,[(betweenness_central_nodes, 'betweenness central node'),\n",
    "                              (pr_result, 'max pagerank'),\n",
    "                              (pr_iso_result, 'max isolated pagerank'),\n",
    "                              (deg_result, 'max degree'),\n",
    "                              (deg_iso_result, 'max isolated degree'),\n",
    "                              (maxx, 'max category')])    \n",
    "    \n",
    "    print('betweenness central nodes visualization')\n",
    "    res.select('cluster', 'LDA topics', 'betweenness central node').show(truncate=False)\n",
    "    print('max pagerank visualization')\n",
    "    res.select('cluster', 'LDA topics', 'max pagerank').show(truncate=False)\n",
    "    print('max pagerank2 visualization')\n",
    "    res.select('cluster', 'LDA topics', 'max isolated pagerank').show(truncate=False)\n",
    "    print('max deg visualization')\n",
    "    res.select('cluster', 'LDA topics', 'max degree').show(truncate=False)\n",
    "    print('max deg2 visualization')\n",
    "    res.select('cluster', 'LDA topics', 'max isolated degree').show(truncate=False)\n",
    "\n",
    "    G = add_attributes_from_df(G_undir, res)\n",
    "    \n",
    "    name = f[12:-5]\n",
    "    path = output_path+name+\"/\"\n",
    "    res.coalesce(1).write.csv(path, mode = 'overwrite')\n",
    "    nx.write_gexf(G, path+name+\".gexf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insider_neighbors_ratio(G, communities, ls):\n",
    "    res = {}\n",
    "    for cluster in ls:\n",
    "        node = ls[cluster]\n",
    "        neighbors = list(nx.classes.function.neighbors(G, node))\n",
    "        community_nodes = communities[cluster]\n",
    "        count = 0\n",
    "    \n",
    "        for n in neighbors:\n",
    "            if n in community_nodes:\n",
    "                count-=-1 #why not ;p\n",
    "    \n",
    "        ratio = \"{0:.2f}\".format(100 * count / len(neighbors))+'%'\n",
    "    \n",
    "        print(cluster, node, ratio)\n",
    "    \n",
    "        res.setdefault(cluster, (node, ratio))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = insider_neighbors_ratio(G_undir, communities, deg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = insider_neighbors_ratio(G_undir, communities, deg_iso_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = insider_neighbors_ratio(G_undir, communities, pr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = insider_neighbors_ratio(G_undir, communities, pr_iso_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''print('betweenness central nodes visualization')\n",
    "res.select('cluster', 'LDA topics', 'betweenness central node').show(truncate=False)\n",
    "print('max pagerank visualization')\n",
    "res.select('cluster', 'LDA topics', 'max pagerank').show(truncate=False)\n",
    "print('max pagerank2 visualization')\n",
    "res.select('cluster', 'LDA topics', 'max isolated pagerank').show(truncate=False)\n",
    "print('max deg visualization')\n",
    "res.select('cluster', 'LDA topics', 'max degree').show(truncate=False)\n",
    "print('max deg2 visualization')\n",
    "res.select('cluster', 'LDA topics', 'max isolated degree').show(truncate=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''import argparse\n",
    "\n",
    "def parseArguments():\n",
    "    # Create argument parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Optional arguments\n",
    "    parser.add_argument(\"-jdk8\", \"--jdk8Path\", help=\"path to jdk8. Default : /usr/lib/jvm/java-1.8.0-openjdk-amd64\", type=str, default='/usr/lib/jvm/java-1.8.0-openjdk-amd64')\n",
    "    parser.add_argument(\"-n4jadr\", \"--neo4jAddress\", help=\"neo4j database address. Default : bolt://localhost:7687\", type=str, default='bolt://localhost:7687')\n",
    "    parser.add_argument(\"-n4jusr\", \"--neo4jUsername\", help=\"neo4j database username. Default : neo4j\", type=str, default='neo4j')\n",
    "    parser.add_argument(\"-n4jpwd\", \"--neo4jPassword\", help=\"neo4j database password. Default : neo4j\", type=str, default='neo4j')\n",
    "    parser.add_argument(\"-ip\", \"--inputPath\", help=\"path of the directory containing the graphs. Default : graphs/\", type=str, default='graphs/')\n",
    "    parser.add_argument(\"-n\", \"--nOfClusters\", help=\"max number of clusters to extract. Default : 20\", type=int, default=20)\n",
    "    parser.add_argument(\"-op\", \"--outputPath\", help=\"path of the output directory. Default : output/\", type=str, default='output/')\n",
    "\n",
    "    # Parse arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "    \n",
    "main(parseArguments())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"I go to the moon\")\n",
    "for e in doc:\n",
    "    print (type(e.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more centrality attributes: pagerank, degrees, and try to find others\n",
    "\n",
    "make code a sort of an executable tool with arguments etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take node with highest degree in each cluster and find its neighbors and seee how many are in the same cluster\n",
    "==> the second variant (using just insiders to compute deg and pagerank) has better ratios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
